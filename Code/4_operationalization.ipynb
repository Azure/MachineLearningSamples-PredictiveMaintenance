{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model operationalization & Deployment\n",
    "\n",
    "In this script, a model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime.\n",
    "\n",
    "**Note:** This notebook will take about 1 minute to execute all cells, depending on the compute configuration you have setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History logging enabled\n",
      "History logging is enabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x7f622700d470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup our environment by importing required libraries\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "\n",
    "# setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# For logging model evaluation parameters back into the\n",
    "# AML Workbench run history plots.\n",
    "import logging\n",
    "from azureml.logging import get_azureml_logger\n",
    "\n",
    "amllog = logging.getLogger(\"azureml\")\n",
    "amllog.level = logging.INFO\n",
    "\n",
    "# Turn on cell level logging.\n",
    "%azureml history on\n",
    "%azureml history show\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run all cells\"\n",
    "tic = time.time()\n",
    "\n",
    "logger = get_azureml_logger() # logger writes to AMLWorkbench runtime view\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Telemetry\n",
    "logger.log('amlrealworld.predictivemaintenance.operationalization','true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the feature data set from memory to construct the operationalization schema. We again will require your storage account name and account key to connect to the blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>dt_truncated</th>\n",
       "      <th>volt_rollingmean_3</th>\n",
       "      <th>rotate_rollingmean_3</th>\n",
       "      <th>pressure_rollingmean_3</th>\n",
       "      <th>vibration_rollingmean_3</th>\n",
       "      <th>volt_rollingmean_24</th>\n",
       "      <th>rotate_rollingmean_24</th>\n",
       "      <th>pressure_rollingmean_24</th>\n",
       "      <th>vibration_rollingmean_24</th>\n",
       "      <th>...</th>\n",
       "      <th>error5sum_rollingmean_24</th>\n",
       "      <th>comp1sum</th>\n",
       "      <th>comp2sum</th>\n",
       "      <th>comp3sum</th>\n",
       "      <th>comp4sum</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "      <th>model_encoded</th>\n",
       "      <th>failure</th>\n",
       "      <th>label_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 06:00:00</td>\n",
       "      <td>147.813753</td>\n",
       "      <td>410.546469</td>\n",
       "      <td>103.110374</td>\n",
       "      <td>39.881874</td>\n",
       "      <td>166.464991</td>\n",
       "      <td>449.921760</td>\n",
       "      <td>100.608971</td>\n",
       "      <td>40.313560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>161.893907</td>\n",
       "      <td>457.866635</td>\n",
       "      <td>106.671660</td>\n",
       "      <td>42.281086</td>\n",
       "      <td>167.917852</td>\n",
       "      <td>459.850110</td>\n",
       "      <td>99.954524</td>\n",
       "      <td>40.198525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>159.216094</td>\n",
       "      <td>466.617543</td>\n",
       "      <td>102.928240</td>\n",
       "      <td>39.135677</td>\n",
       "      <td>169.175332</td>\n",
       "      <td>456.416658</td>\n",
       "      <td>99.402692</td>\n",
       "      <td>39.688645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 21:00:00</td>\n",
       "      <td>173.141342</td>\n",
       "      <td>466.089834</td>\n",
       "      <td>102.410363</td>\n",
       "      <td>40.737921</td>\n",
       "      <td>170.269608</td>\n",
       "      <td>453.365861</td>\n",
       "      <td>97.793726</td>\n",
       "      <td>39.614332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 18:00:00</td>\n",
       "      <td>173.328305</td>\n",
       "      <td>445.790528</td>\n",
       "      <td>96.623228</td>\n",
       "      <td>39.309750</td>\n",
       "      <td>168.427467</td>\n",
       "      <td>452.489297</td>\n",
       "      <td>96.946852</td>\n",
       "      <td>39.826918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID        dt_truncated  volt_rollingmean_3  rotate_rollingmean_3  \\\n",
       "0         27 2016-01-01 06:00:00          147.813753            410.546469   \n",
       "1         27 2016-01-01 03:00:00          161.893907            457.866635   \n",
       "2         27 2016-01-01 00:00:00          159.216094            466.617543   \n",
       "3         27 2015-12-31 21:00:00          173.141342            466.089834   \n",
       "4         27 2015-12-31 18:00:00          173.328305            445.790528   \n",
       "\n",
       "   pressure_rollingmean_3  vibration_rollingmean_3  volt_rollingmean_24  \\\n",
       "0              103.110374                39.881874           166.464991   \n",
       "1              106.671660                42.281086           167.917852   \n",
       "2              102.928240                39.135677           169.175332   \n",
       "3              102.410363                40.737921           170.269608   \n",
       "4               96.623228                39.309750           168.427467   \n",
       "\n",
       "   rotate_rollingmean_24  pressure_rollingmean_24  vibration_rollingmean_24  \\\n",
       "0             449.921760               100.608971                 40.313560   \n",
       "1             459.850110                99.954524                 40.198525   \n",
       "2             456.416658                99.402692                 39.688645   \n",
       "3             453.365861                97.793726                 39.614332   \n",
       "4             452.489297                96.946852                 39.826918   \n",
       "\n",
       "    ...     error5sum_rollingmean_24  comp1sum  comp2sum  comp3sum  comp4sum  \\\n",
       "0   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "1   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "2   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "3   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "4   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "\n",
       "    model  age    model_encoded  failure  label_e  \n",
       "0  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "1  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "2  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "3  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "4  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# We will create this container to hold the results of executing this notebook.\n",
    "# If this container name already exists, we will use that instead, however\n",
    "# This notebook will ERASE ALL CONTENTS.\n",
    "CONTAINER_NAME = \"featureengineering\"\n",
    "FE_DIRECTORY = 'featureengineering_files.parquet'\n",
    "\n",
    "MODEL_CONTAINER = 'modeldeploy'\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(CONTAINER_NAME, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "if not os.path.exists(FE_DIRECTORY):\n",
    "    os.makedirs(FE_DIRECTORY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if CONTAINER_NAME in blob.name:\n",
    "        local_file = os.path.join(FE_DIRECTORY, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "fedata = spark.read.parquet(FE_DIRECTORY)\n",
    "\n",
    "fedata.limit(5).toPandas().head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define init and run functions\n",
    "Start by defining the init() and run() functions as shown in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "\n",
    "The init() function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker container containing your service initializes.\n",
    "The run() function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    \n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        \n",
    "        # We'll use the known label, key variables and \n",
    "        # a few extra columns we won't need.\n",
    "        key_cols =['label_e','machineID','dt_truncated', 'failure','model_encoded','model' ]\n",
    "\n",
    "        # Then get the remaing feature names from the data\n",
    "        input_features = input_df.columns\n",
    "\n",
    "        # Remove the extra stuff if it's in the input_df\n",
    "        input_features = [x for x in input_features if x not in set(key_cols)]\n",
    "        \n",
    "        # Vectorize as in model building\n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create schema and schema file\n",
    "Create a schema for the input to the web service and generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, \n",
    "                                       fedata.drop(\"dt_truncated\",\"failure\",\"label_e\", \"model\",\"model_encoded\"))}\n",
    "\n",
    "json_schema = generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test init and run\n",
    "We can then test the init() and run() functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"0.0\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll use the known label, key variables and \n",
    "# a few extra columns we won't need. (machineID is required)\n",
    "key_cols =['label_e','dt_truncated', 'failure','model_encoded','model' ]\n",
    "\n",
    "# Then get the remaining feature names from the data\n",
    "input_features = fedata.columns\n",
    "# Remove the extra stuff if it's in the input_df\n",
    "input_features = [x for x in input_features if x not in set(key_cols)]\n",
    "\n",
    "\n",
    "# this is an example input data record\n",
    "input_data = [[114, 163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,\n",
    "               277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,\n",
    "               4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,\n",
    "               0.0,0.0,489.0,549.0,549.0,564.0,18.0]]\n",
    "\n",
    "df = (spark.createDataFrame(input_data, input_features))\n",
    "\n",
    "# test init() in local notebook\n",
    "init()\n",
    "\n",
    "# test run() in local notebook\n",
    "run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "\n",
    "Next we persist the assets we have created to disk for use in operationalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use `%%writefile` meta command to save the `init()` and `run()` functions to the save the `pdmscore.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /azureml-share//pdmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']}/pdmscore.py\n",
    "\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    # read in the model file\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load('pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    response = ''\n",
    "    try:\n",
    "       \n",
    "        # We'll use the known label, key variables and \n",
    "        # a few extra columns we won't need.\n",
    "        key_cols =['label_e','machineID','dt_truncated', 'failure','model_encoded','model' ]\n",
    "\n",
    "        # Then get the remaing feature names from the data\n",
    "        input_features = input_df.columns\n",
    "\n",
    "        # Remove the extra stuff if it's in the input_df\n",
    "        input_features = [x for x in input_features if x not in set(key_cols)]\n",
    "        \n",
    "        # Vectorize as in model building\n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"input_df\\\":[{\\\"machineID\\\":114,\\\"volt_rollingmean_3\\\":163.375732902,\\\"rotate_rollingmean_3\\\":333.149484586,\\\"pressure_rollingmean_3\\\":100.183951698,\\\"vibration_rollingmean_3\\\":44.0958812638,\\\"volt_rollingmean_24\\\":164.114723991,\\\"rotate_rollingmean_24\\\":277.191815232,\\\"pressure_rollingmean_24\\\":97.6289110707,\\\"vibration_rollingmean_24\\\":50.8853505161,\\\"volt_rollingstd_3\\\":21.0049565219,\\\"rotate_rollingstd_3\\\":67.5287259378,\\\"pressure_rollingstd_3\\\":12.9361526861,\\\"vibration_rollingstd_3\\\":4.61359760918,\\\"volt_rollingstd_24\\\":15.5377738062,\\\"rotate_rollingstd_24\\\":67.6519885441,\\\"pressure_rollingstd_24\\\":10.528274633,\\\"vibration_rollingstd_24\\\":6.94129487555,\\\"error1sum_rollingmean_24\\\":0.0,\\\"error2sum_rollingmean_24\\\":0.0,\\\"error3sum_rollingmean_24\\\":0.0,\\\"error4sum_rollingmean_24\\\":0.0,\\\"error5sum_rollingmean_24\\\":0.0,\\\"comp1sum\\\":489.0,\\\"comp2sum\\\":549.0,\\\"comp3sum\\\":549.0,\\\"comp4sum\\\":564.0,\\\"age\\\":18.0}]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are stored in the `['AZUREML_NATIVE_SHARE_DIRECTORY']` location on the kernel host machine with the model stored in the `3_model_building.ipynb` notebook. In order to share these assets and operationalize the model, we create a new blob container and store a compressed file containing those assets for later retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full run took 0.68 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x7f6226fc3048>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "MODEL_O16N = shutil.make_archive('o16n', 'zip', os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'])\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"o16n.zip\", str(MODEL_O16N) ) \n",
    "\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "toc = time.time()\n",
    "print(\"Full run took %.2f minutes\" % ((toc - tic)/60))\n",
    "\n",
    "logger.log(\"Operationalization Run time\", ((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a local compute context for operationalization on an Azure web service.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "### Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    "- Open the [Azure Portal](http://portal.azure.com)\n",
    "- In the left hand pane, click on __All resources__\n",
    "- Search for the storage account using the name you provided earlier in this notebook. \n",
    "- Choose the storage account from search result list, this will open the storage account panel.\n",
    "- On the storage account panel, choose __Blobs__\n",
    "- On the Blobs panel choose the container __modeldeploy__\n",
    "- Select the file o16n.zip and on the properties pane for that blob choose download.\n",
    "\n",
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains three deployment assets:\n",
    "\n",
    "- the `pdmscore.py` file\n",
    "- a `pdmrfull.model` directory\n",
    "- the `service_schema.json` file\n",
    "\n",
    "\n",
    "\n",
    "### Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `pdmmodelmanagement`. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "\n",
    "### Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with the existing model management context first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "\n",
    "### Deploy your web service \n",
    "\n",
    "Once the environment is setup, we'll deploy the web service from the CLI.\n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (`pdmscore.py`, `service_schema.json` and `pdmrfull.model`). If your kernel has run locally, the assets will be in the `os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']`. \n",
    "\n",
    "On windows this points to:\n",
    "\n",
    "```\n",
    "cd C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name>\n",
    "```\n",
    "\n",
    "on linux variants this points to:\n",
    "\n",
    "```\n",
    "cd ~\\.azureml\\share\\<team account>\\<Project Name>\n",
    "```\n",
    "\n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "```\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "```\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice `amlworkbenchpdmwebservice`. This `SERVICE_ID` must be all lowercase, with no spaces:\n",
    "\n",
    "```\n",
    "az ml service create realtime -f pdmscore.py -r spark-py -m pdmrfull.model -s service_schema.json --cpu 0.1 -n amlworkbenchpdmwebservice\n",
    "```\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "Once complete, the command returns sample usage commands to test the service for both PowerShell and the cmd prompt. We can execute these commands from the command line as well. For our example:\n",
    "\n",
    "```\n",
    "az ml service run realtime -i amlworkbenchpdmwebservice --% -d \"{\\\"input_df\\\": [{\\\"rotate_rollingstd_24\\\": 0.3233426394949046, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"age\\\": 14, \\\"machineID\\\": 45, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_24\\\": 0.1945085296751734, \\\"vibration_rollingstd_24\\\": 0.36239263228769986, \\\"rotate_rollingmean_3\\\": 527.816906803798, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"volt_rollingmean_24\\\": 185.92637096180658, \\\"pressure_rollingmean_3\\\": 117.22597085550017, \\\"volt_rollingstd_24\\\": 0.03361414142292652, \\\"comp1sum\\\": 474.0, \\\"comp3sum\\\": 384.0, \\\"pressure_rollingmean_24\\\": 113.56479908060074, \\\"rotate_rollingstd_3\\\": 2.2898301915618045, \\\"volt_rollingmean_3\\\": 174.88172665757065, \\\"comp2sum\\\": 459.0, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"rotate_rollingmean_24\\\": 470.1219658987775, \\\"vibration_rollingmean_3\\\": 39.472146777953654, \\\"vibration_rollingstd_3\\\": 0.8102848856599294, \\\"pressure_rollingstd_3\\\": 0.010565393835276299, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"volt_rollingstd_3\\\": 8.308641250692387, \\\"vibration_rollingmean_24\\\": 39.93637676066078, \\\"comp4sum\\\": 579.0}, {\\\"rotate_rollingstd_24\\\": 1.5152162169310932, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"age\\\": 14, \\\"machineID\\\": 45, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_24\\\": 0.012495480312639678, \\\"vibration_rollingstd_24\\\": 0.21106710997624312, \\\"rotate_rollingmean_3\\\": 474.63178724391287, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"volt_rollingmean_24\\\": 186.1033733765524, \\\"pressure_rollingmean_3\\\": 124.26190112949568, \\\"volt_rollingstd_24\\\": 0.7740120822459206, \\\"comp1sum\\\": 474.0, \\\"comp3sum\\\": 384.0, \\\"pressure_rollingmean_24\\\": 112.46729566613514, \\\"rotate_rollingstd_3\\\": 13.920898245623066, \\\"volt_rollingmean_3\\\": 188.406673928196, \\\"comp2sum\\\": 459.0, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"rotate_rollingmean_24\\\": 461.1030486200735, \\\"vibration_rollingmean_3\\\": 38.869583185731614, \\\"vibration_rollingstd_3\\\": 1.9805973022526275, \\\"pressure_rollingstd_3\\\": 1.7895872952762106, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"volt_rollingstd_3\\\": 4.60785082568852, \\\"vibration_rollingmean_24\\\": 39.96976455089771, \\\"comp4sum\\\": 579.0}, {\\\"rotate_rollingstd_24\\\": 2.017971138478601, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"age\\\": 14, \\\"machineID\\\": 45, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_24\\\": 0.2620300574897778, \\\"vibration_rollingstd_24\\\": 0.16523682934622702, \\\"rotate_rollingmean_3\\\": 454.8717742309143, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"volt_rollingmean_24\\\": 184.4934951791266, \\\"pressure_rollingmean_3\\\": 123.02912082922734, \\\"volt_rollingstd_24\\\": 0.4103068092842077, \\\"comp1sum\\\": 473.6666666666667, \\\"comp3sum\\\": 383.6666666666667, \\\"pressure_rollingmean_24\\\": 110.24028050598271, \\\"rotate_rollingstd_3\\\": 15.91959183377542, \\\"volt_rollingmean_3\\\": 171.32900821497492, \\\"comp2sum\\\": 458.6666666666667, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"rotate_rollingmean_24\\\": 458.14752146073414, \\\"vibration_rollingmean_3\\\": 37.71234613693027, \\\"vibration_rollingstd_3\\\": 2.3594190696788924, \\\"pressure_rollingstd_3\\\": 1.808640841551748, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"volt_rollingstd_3\\\": 7.16544669362819, \\\"vibration_rollingmean_24\\\": 39.621269267841434, \\\"comp4sum\\\": 578.6666666666666}]}\"\n",
    "```\n",
    "\n",
    "This submits 3 records to the model through the web service, and returns predictioned output labels for each of the three rows:\n",
    "```\n",
    "\"0.0,0.0,0.0\"\n",
    "```\n",
    "\n",
    "Indicating that these records are not predicted to fail with in the requested time.\n",
    "\n",
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredictiveMaintenance dockerDSVM",
   "language": "python",
   "name": "predictivemaintenance_dockerdsvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
